{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# if len(sys.argv) != 2:\n",
    "# \tsys.exit(\"Use: python train.py <dataset>\")\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr', 'twitter', \"twitter_hate_off\"]\n",
    "dataset = \"twitter_hate_off\"\n",
    "\n",
    "if dataset not in datasets:\n",
    "\tsys.exit(\"wrong dataset name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a9b608b65536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n\u001b[0;32m---> 27\u001b[0;31m     FLAGS.dataset)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# print(adj)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 633\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = random.randint(1, 200)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "# 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('dataset', dataset, 'Dataset string.')\n",
    "# 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
    "flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 400, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.8, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0,\n",
    "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
    "flags.DEFINE_integer('early_stopping', 10,\n",
    "                     'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 2, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n",
    "    FLAGS.dataset)\n",
    "# print(adj)\n",
    "\n",
    "print(\"Features shape :: \" , features.shape)\n",
    "print(\"Features shape_0 :: \" , features.shape[0])\n",
    "# print(\"Features shape :: \" , features.shape)\n",
    "features = sp.identity(features.shape[0])  # featureless\n",
    "\n",
    "# glove features\n",
    "\n",
    "print(test_mask)\n",
    "\n",
    "\n",
    "# exit(0)\n",
    "print('embeddings:')\n",
    "def loadGloveModel(gloveFile,words):\n",
    "    print(\"Loading Glove Model\")\n",
    "    # path = os.path.join(\"data\", \"gcn_glove_\" + dataset + \".json\")\n",
    "    # if os.path.exists(path):\n",
    "    #     with open(path, \"r\") as f:\n",
    "    #         return json.load(f)\n",
    "\n",
    "    f = open(gloveFile,'r')\n",
    "\n",
    "    all_words = set()\n",
    "    for line in words:\n",
    "        for word in line.split():\n",
    "            all_words.add(word)\n",
    "    \n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0].strip().strip('<').strip('>')\n",
    "        if word in all_words:\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded! Actual : \", len(all_words))\n",
    "    \n",
    "    # with open(path, \"w\") as f:\n",
    "    #     json.dump(model, f)\n",
    "    return model\n",
    "\n",
    "dim = 100\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'r')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "glove_file_name = \"./data/glove.twitter.27B.\"+ str(dim) +\"d.txt\"\n",
    "glove_vectors = loadGloveModel(glove_file_name, words)\n",
    "glove_embeddings = np.random.normal(size = (adj.shape[0], dim))\n",
    "glove_embeddings[:train_size] = np.zeros(shape = (train_size, dim))\n",
    "glove_embeddings[adj.shape[0] - test_size:] = np.zeros(shape = (test_size, dim))\n",
    "\n",
    "for i in range(len(words)):\n",
    "    word = words[i].strip()\n",
    "    if word in glove_vectors:\n",
    "        glove_embeddings[i + train_size, :] = glove_vectors[word]\n",
    "    else:\n",
    "        glove_embeddings[i + train_size] = np.random.normal(size = [dim])\n",
    "\n",
    "\n",
    "word_embs = np.array(glove_embeddings)\n",
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "print(word_embs.shape)\n",
    "\n",
    "# import cPickle as cp \n",
    "# with open('feature.data') as f:\n",
    "#     cp.dump(features, f)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    # helper variable for sparse dropout\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "print(\"\\n\\n\\n\\n\\nCreate model\\n\\n\\n\")\n",
    "print(features[2][1])\n",
    "model = model_func(\n",
    "    placeholders, input_dim=features[2][1], logging=True, word_emb=word_embs.astype(np.float32), train_size = train_size, test_size = test_size)\n",
    "\n",
    "# Initialize session\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=session_conf)\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(\n",
    "        features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels, model.hidden1], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], outs_val[4],(time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(\n",
    "        features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
    "                     model.hidden1], feed_dict=feed_dict)\n",
    "    # Validation\n",
    "    cost, acc, pred, labels, repre,duration = evaluate(\n",
    "        features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(\n",
    "              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "# Training\n",
    "train_cost, train_acc, pred, labels,t_representation, train_duration = evaluate(\n",
    "    features, support, y_train, train_mask, placeholders)\n",
    "print(\"\\n\\n\\n\\nTrain set results:\", \"cost=\", \"{:.5f}\".format(train_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(train_acc), \"time=\", \"{:.5f}\".format(train_duration))\n",
    "\n",
    "train_pred = []\n",
    "train_labels = []\n",
    "print(len(train_mask))\n",
    "for i in range(len(train_mask)):\n",
    "    if train_mask[i]:\n",
    "        train_pred.append(pred[i])\n",
    "        train_labels.append(labels[i])\n",
    "\n",
    "print(\"Train Precision, Recall and F1-Score...\")\n",
    "print(metrics.classification_report(train_labels, train_pred, digits=4))\n",
    "print(\"Macro average Train Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(train_labels, train_pred, average='macro'))\n",
    "print(\"Micro average Train Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(train_labels, train_pred, average='micro'))\n",
    "print(\"Weight average Train Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(train_labels, train_pred, average='weighted'))\n",
    "\n",
    "\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, pred, labels,representation, test_duration = evaluate(\n",
    "    features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "print(len(test_mask))\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(representation)\n",
    "fp = open(\"out_pred.txt\", \"w\")\n",
    "for i,j in zip(test_pred, test_labels):\n",
    "    fp.write(str(i) + \",\" + str(j) + \"\\n\")\n",
    "fp.close()\n",
    "\n",
    "print(metrics.confusion_matrix(test_labels,test_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
    "print(\"Weight average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.6MB 496kB/s ta 0:00:011    45% |██████████████▋                 | 42.3MB 1.2MB/s eta 0:00:43\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.4MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/sud/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.4)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 578kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/7b/3ee06856ec30d5136cd2002408df1d111fcff269f3691147dbf3b8dc0ba2/tensorboard-1.13.0-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/sud/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/92/0637b1b0e646348bd0e8bd2796dcb5434a21711eff54a456617fc7df3916/protobuf-3.7.0-cp37-cp37m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 1.1MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/sud/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.32.3)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/60/b58313c04365bd2b3c63d077bedb9c28723b9b22b399d3e89c96a16bc5bc/grpcio-1.19.0-cp37-cp37m-manylinux1_x86_64.whl (10.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.8MB 5.6MB/s eta 0:00:01  25% |████████                        | 2.7MB 808kB/s eta 0:00:11\n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in /home/sud/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/sud/anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /home/sud/anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl (107kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gast, termcolor, absl-py\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/sud/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/sud/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/sud/.cache/pip/wheels/90/db/f8/2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\n",
      "Successfully built gast termcolor absl-py\n",
      "Installing collected packages: absl-py, pbr, mock, tensorflow-estimator, keras-preprocessing, gast, termcolor, keras-applications, protobuf, grpcio, markdown, tensorboard, astor, tensorflow\n",
      "Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.0.1 mock-2.0.0 pbr-5.1.3 protobuf-3.7.0 tensorboard-1.13.0 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
